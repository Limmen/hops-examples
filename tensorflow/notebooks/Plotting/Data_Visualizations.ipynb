{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting With SparkMagic on Hops\n",
    "\n",
    "To run large scale computations in a hops cluster from Jupyter we use sparkmagic, a livy REST server, and the pyspark kernel. \n",
    "\n",
    "The fact that the default computation on a cluster is distributed over several machines makes it a little different to do things such as plotting compared to when running code locally. \n",
    "\n",
    "This notebook illustrates how you can combine plotting and large-scale computations on a Hops cluster in a single notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When pyspark kernel is started we get a Spark session automatically created for us\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Check which \"magic\" functions are available from sparkmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a CSV file in Spark from your Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hops import hdfs\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(hdfs.project_path() + \"TestJob/data/visualization/Pokemon.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name the Spark DataFrame to Be Able to Use SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"pokemons\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use SparkMagic to Collect the Spark Dataframe as a Pandas Dataframe Locally\n",
    "\n",
    "This command will send the dataset from the cluster to the server where Jupyter is running and convert it into a pandas dataframe. This is only suitable for smaller datasets. A common practice is to run spark jobs to process a large dataset and shrink it before plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql -c sql -o python_df --maxrows 10\n",
    "SELECT * FROM pokemons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Pandas DataFrame is now Available in %%local mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "python_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "python_df[\"Name\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install --user matplotlib\n",
    "pip install --user seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Plotting with MatplotLib and Seaborn\n",
    "\n",
    "After the data have been loaded locally as a pandas dataframe, it can get plotted on the Jupyter server. By using the magic \"%%local\" at the top of a cell, the code in the cell will be executed locally on the Jupyter server, rather than remotely with Livy on the Spark cluster. Once the pandas dataframe is available locally it can be plotted with libraries such as matplotlib and seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "stats = python_df.columns[5:11]\n",
    "plt.figure(figsize=(25, 20))\n",
    "\n",
    "for ii, stat in enumerate(stats):\n",
    "    title = \"Distributions of {stat}\".format(\n",
    "        stat = stat\n",
    "    )\n",
    "    plt.subplot(3, 3, ii+1)\n",
    "    plt.title(title)\n",
    "    stats = np.array(map(lambda x: int(x), python_df[stat].values))\n",
    "    sns.distplot(stats)\n",
    "    x = plt.gca().get_xlim()[1] * .6\n",
    "    y = plt.gca().get_ylim()[1] * .9\n",
    "    plt.text(x, y, '$\\mu: {mu: .2f}, \\sigma: {sigma: .2f}$'.format(mu = stats.mean(), sigma=stats.std()))\n",
    "    \n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "#view the nuumber of pokemons for Type 1 and Type 2 using one plot\n",
    "f, (ax1,ax2) = plt.subplots(2,1,figsize=(15, 8),sharex=True)\n",
    "\n",
    "sns.countplot('Type 1',data=python_df,ax=ax1)\n",
    "sns.countplot('Type 2',data=python_df,ax=ax2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "sns.catplot(x='Legendary',kind='count',data=python_df,height=5,aspect=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
