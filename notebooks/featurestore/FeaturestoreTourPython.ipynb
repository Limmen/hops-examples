{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Feature Store Tour - Python API\n",
    " \n",
    "This notebook contains a tour/reference for the feature store Python API on hopsworks. The notebook is ment to be run from feature store demo projects on Hopsworks. We will go over best practices for using the API as well as common pitfalls.\n",
    " \n",
    "The notebook is designed to be used in combination with the Feature Store Tour on Hopsworks, it assumes that you have run the following feature engineering job: [job](https://github.com/logicalclocks/hops-examples/tree/master/featurestore) (**the job is added automatically when you start the feature store tour in Hopsworks. You can run the job by going to the 'Jobs' tab to the left in the Hopsworks project home page**). \n",
    "\n",
    "Which will produce the following model of feature groups in your project's feature store:\n",
    "\n",
    "![Feature Store Model](./images/model.png \"Feature Store Model\")\n",
    "\n",
    "In this notebook we will run queries over this feature store model. We will also create new feature groups and training datasets.\n",
    "\n",
    "We will go from (1) features to (2) training datasets to (3) A trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Store 101\n",
    "\n",
    "The simplest way to think about the feature store is as a central place to store curated /features/ within an organization. A feature is a measurable property of some phenomenon. It could be for example an image-pixel, a word from a piece of text, the age of a person, a coordinate emitted from a sensor, or an aggregate value like the average number of purchases within the last hour.\n",
    "\n",
    "A feature store is a data management layer for machine learning that can optimize the machine learning workflow and provide an interface between data engineering and data science.\n",
    "\n",
    "![Feature Store Overview](./images/overview.png \"Feature Store Overview\")\n",
    "\n",
    "A feature store is not a pure storage service, it goes hand-in-hand with feature computation. Feature engineering is the process of transforming raw data into a format that is compatible and understandable for predictive models.\n",
    "\n",
    "There are two interfaces to the feature store:\n",
    "\n",
    "- Writing to the feature store, at the end of the feature engineering pipeline the features are written to the feature store, e.g:\n",
    "```python\n",
    "raw_data = spark.read.format(\"csv\").load(filename)\n",
    "polynomial_features = raw_data.map(lambda x: x^2)\n",
    "from hops import featurestore\n",
    "featurestore.insert_into_featuregroup(polynomial_features, \"polynomial_features\")\n",
    "```\n",
    "- Reading from the feature store, to train a model on a set of features, the features can be read from the feature store, e.g:\n",
    "```python\n",
    "from hops import featurestore\n",
    "features_df = featurestore.get_features([\"team_budget\", \"average_attendance\", \"average_player_age\"])\n",
    "```\n",
    "\n",
    "As a data engineer/data scientist, you can think of the feature store as a middle-layer. Once you have computed a set of features, instead of writing them locally to a csv file, insert them in the feature store so that the features can get documented/versioned, backfilled, **and so that your colleagues can re-use your features!** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hops library is automatically installed in all Hopsworks-projects. This library has a module called `featurestore` that provides an API for the feature store. You can find API documentation [here](http://hops-py.logicalclocks.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>25</td><td>application_1555980784524_0028</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hopsworks0.logicalclocks.com:8088/proxy/application_1555980784524_0028/\">Link</a></td><td><a target=\"_blank\" href=\"http://hopsworks0.logicalclocks.com:8042/node/containerlogs/container_e01_1555980784524_0028_01_000001/demo_featurestore_admin000__meb10000\">Link</a></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "from hops import featurestore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get The Name of The Project's Feature Store\n",
    "\n",
    "Each project with the feature store service enabled automatically gets its own feature store created. This feature store is only accessible within the project unless you decide to share it with other projects. The name of the feature store is `<project_name>_featurestore`, and you can get the name with the API method `project_featurestore()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'demo_featurestore_admin000_featurestore'"
     ]
    }
   ],
   "source": [
    "featurestore.project_featurestore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get a List of All Feature Stores Accessible in the Current Project \n",
    "\n",
    "Feature Stores can be shared across projects in a multi-tenant manner, just like any Hopsworks-dataset can. You can read more about sharing datasets at [hops.io](hops.io), but in essence to share a dataset you just have to right click on it in your project. The feature groups in the feature store are located in a dataset called `<project_name>_featurestore.db` in your project.\n",
    "\n",
    "![Share Feature Store](./images/share_featurestore.png \"Share Feature Store\")\n",
    "\n",
    "The Training Datasets in your feature store are located in a datased called `<project_name>_Training_Datasets` inside your project. Typically if you want to share a feature store with another project you will share both the `<project_name>_featurestore.db` dataset and the `<project_name>_Training_Datasets` dataset.\n",
    "\n",
    "![Share Feature Store](./images/share_featurestore2.png \"Share Feature Store\")\n",
    "\n",
    "To list all feature stores accessible in the current project, you can use the API method `get_project_featurestores()`. You can also view the list of accessible feature stores in the feature registry UI:\n",
    "\n",
    "![Share Feature Store](./images/select_fs.png \"Share Feature Store\")\n",
    "\n",
    "By using multiple feature stores and feature store sharing across projects you can enforce access rights to features.\n",
    "\n",
    "![Multi-Tenant Feature Stores](./images/multitenant.png \"Multi-Tenant Feature Stores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['demo_featurestore_admin000_featurestore']"
     ]
    }
   ],
   "source": [
    "featurestore.get_project_featurestores()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying The Feature Store\n",
    "\n",
    "The feature store can be queried programmatically and with raw SQL. When you query the feature store programmatically, the library will infer how to fetch the different features using a **query planner**. \n",
    "\n",
    "![Feature Store Query Planner](./images/query_optimizer.png \"Feature Store Query Planner\")\n",
    "\n",
    "When interacting with the feature store it is sufficient to be familiar with three concepts:\n",
    "\n",
    "- The **feature**, this refer to an individual versioned and documented feature in the feature store, e.g the age of a person.\n",
    "- The **feature group**, this refer to a documented and versioned group of features stored as a Hive table that is linked to a specific Spark/Numpy/Pandas job that takes in raw data and outputs the computed features.\n",
    "- The **training dataset**, this refer to a versioned and managed dataset of features, stored in HopsFS as tfrecords, .csv, .tsv, or parquet.\n",
    "\n",
    "A feature group contains a group of features and a training dataset contains a set of features, potentially from many different feature groups.\n",
    "\n",
    "![Feature Store Concepts](./images/concepts.png \"Feature Store Contents\")\n",
    "\n",
    "When you query the feature store you will always get back the results in a spark dataframe. This is for scalability reasons. If the dataset is small and you want to work with it in memory you can convert it into a pandas dataframe or a numpy matrix using one line of code as we will demonstrate later on in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch an Individual Feature\n",
    "\n",
    "When retrieving a single feature from the featurestore, the hops-util-py library will infer in which feature group the feature belongs to by querying the metastore, but you can also explicitly specify which featuregroup and version to query. \n",
    "\n",
    "If there are multiple features of the same name in the featurestore, it is required to specify enough information to uniquely identify the feature (e.g specify feature group and version). If no featurestore is provided it will default to the project's featurestore.\n",
    "\n",
    "To read an individual feature, use the method `get_feature(feature_name)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without specifying the feature store, feature group and version, the library will infer it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sql: use demo_featurestore_admin000_featurestore\n",
      "Running sql: SELECT team_budget FROM teams_features_1\n",
      "+-----------+\n",
      "|team_budget|\n",
      "+-----------+\n",
      "|  12957.076|\n",
      "|  2403.3704|\n",
      "|  3390.3755|\n",
      "|  13547.429|\n",
      "|   9678.333|\n",
      "+-----------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "featurestore.get_feature(\"team_budget\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also explicitly specify the feature store, feature group, the version, and the return format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sql: use demo_featurestore_admin000_featurestore\n",
      "Running sql: SELECT team_budget FROM teams_features_1\n",
      "+-----------+\n",
      "|team_budget|\n",
      "+-----------+\n",
      "|  12957.076|\n",
      "|  2403.3704|\n",
      "|  3390.3755|\n",
      "|  13547.429|\n",
      "|   9678.333|\n",
      "+-----------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "featurestore.get_feature(\n",
    "    \"team_budget\", \n",
    "    featurestore=featurestore.project_featurestore(), \n",
    "    featuregroup=\"teams_features\", \n",
    "    featuregroup_version = 1,\n",
    "    dataframe_type = \"spark\"\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch an Entire Feature Group\n",
    "\n",
    "You can get an entire featuregroup from the API. If no feature store is provided the API will default to the project's feature store, if no version is provided it will default to version 1 of the feature group. The default return format is as a spark dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sql: use demo_featurestore_admin000_featurestore\n",
      "Running sql: SELECT * FROM teams_features_1\n",
      "+-----------+-------+-------------+\n",
      "|team_budget|team_id|team_position|\n",
      "+-----------+-------+-------------+\n",
      "|  12957.076|      1|            1|\n",
      "|  2403.3704|      2|            2|\n",
      "|  3390.3755|      3|            3|\n",
      "|  13547.429|      4|            4|\n",
      "|   9678.333|      5|            5|\n",
      "+-----------+-------+-------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "featurestore.get_featuregroup(\"teams_features\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default parameters can be overriden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sql: use demo_featurestore_admin000_featurestore\n",
      "Running sql: SELECT * FROM teams_features_1\n",
      "+-----------+-------+-------------+\n",
      "|team_budget|team_id|team_position|\n",
      "+-----------+-------+-------------+\n",
      "|  12957.076|      1|            1|\n",
      "|  2403.3704|      2|            2|\n",
      "|  3390.3755|      3|            3|\n",
      "|  13547.429|      4|            4|\n",
      "|   9678.333|      5|            5|\n",
      "+-----------+-------+-------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "featurestore.get_featuregroup(\n",
    "    \"teams_features\", \n",
    "    featurestore=featurestore.project_featurestore(), \n",
    "    featuregroup_version = 1,\n",
    "    dataframe_type = \"spark\"\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch A Set of Features\n",
    "\n",
    "When retrieving a list of features from the featurestore, the hops-util-py library will infer which featuregroup the features belongs to by querying the metastore. If the features reside in different featuregroups, the library will also try to infer how to join the features together based on common columns. If the JOIN query cannot be inferred due to existence of multiple features with the same name or non-obvious JOIN query, the user need to supply enough information to the API call to be able to query the featurestore. If the user already knows the JOIN query it can also run featurestore.sql(joinQuery) directly (an example of this is shown further down in this notebook). If no featurestore is provided the API will default to the project's featurestore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of querying the feature store for a list of features without specifying the feature groups and feature store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sql: use demo_featurestore_admin000_featurestore\n",
      "Running sql: SELECT average_player_age, team_budget, average_attendance FROM players_features_1 JOIN teams_features_1 JOIN attendances_features_1 ON players_features_1.`team_id`=teams_features_1.`team_id` AND players_features_1.`team_id`=attendances_features_1.`team_id`\n",
      "+------------------+-----------+------------------+\n",
      "|average_player_age|team_budget|average_attendance|\n",
      "+------------------+-----------+------------------+\n",
      "|             24.85|    7307.94|         19595.973|\n",
      "|             25.45|   7326.092|          6462.462|\n",
      "|              25.4|   3555.235|          7226.672|\n",
      "|             25.91|  910.39325|         3189.8455|\n",
      "|             25.78|  12474.419|          9405.213|\n",
      "+------------------+-----------+------------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "featurestore.get_features(\n",
    "    [\"team_budget\", \"average_attendance\", \"average_player_age\"]\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also explicitly specify the feature groups where the features reside. Either the feature groups and versions can be specified by prepending feature names with `<feature group name>_<feature group version.`, or by providing a dict with entries of `<feature group name> -> <feature group version>`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sql: use demo_featurestore_admin000_featurestore\n",
      "Running sql: SELECT attendances_features_1.average_attendance, teams_features_1.team_budget, players_features_1.average_player_age FROM attendances_features_1 JOIN teams_features_1 JOIN players_features_1 ON attendances_features_1.`team_id`=teams_features_1.`team_id` AND attendances_features_1.`team_id`=players_features_1.`team_id`\n",
      "+------------------+-----------+------------------+\n",
      "|average_attendance|team_budget|average_player_age|\n",
      "+------------------+-----------+------------------+\n",
      "|          3271.934|  16758.066|             25.65|\n",
      "|         4074.8047|  3966.3591|              25.5|\n",
      "|         19595.973|    7307.94|             24.85|\n",
      "|          6462.462|   7326.092|             25.45|\n",
      "|          7226.672|   3555.235|              25.4|\n",
      "+------------------+-----------+------------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "featurestore.get_features(\n",
    "    [\"teams_features_1.team_budget\", \n",
    "     \"attendances_features_1.average_attendance\", \n",
    "     \"players_features_1.average_player_age\"]\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sql: use demo_featurestore_admin000_featurestore\n",
      "Running sql: SELECT average_player_age, team_budget, average_attendance FROM attendances_features_1 JOIN players_features_1 JOIN teams_features_1 ON attendances_features_1.`team_id`=players_features_1.`team_id` AND attendances_features_1.`team_id`=teams_features_1.`team_id`\n",
      "+------------------+-----------+------------------+\n",
      "|average_player_age|team_budget|average_attendance|\n",
      "+------------------+-----------+------------------+\n",
      "|             24.85|    7307.94|         19595.973|\n",
      "|             25.45|   7326.092|          6462.462|\n",
      "|              25.4|   3555.235|          7226.672|\n",
      "|             25.91|  910.39325|         3189.8455|\n",
      "|             25.78|  12474.419|          9405.213|\n",
      "+------------------+-----------+------------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "featurestore.get_features(\n",
    "    [\"team_budget\", \"average_attendance\", \"average_player_age\"],\n",
    "    featurestore=featurestore.project_featurestore(),\n",
    "    featuregroups_version_dict={\n",
    "        \"teams_features\": 1, \n",
    "        \"attendances_features\": 1,\n",
    "        \"players_features\": 1\n",
    "    }\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a lot of name collisions and it is not obvious how to infer the JOIN query to get the features from the feature store. You can explicitly specify the argument `join_key` to the API (or you can provide the entire SQL query using the API method `.sql` as we will demonstrate later on in the notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sql: use demo_featurestore_admin000_featurestore\n",
      "Running sql: SELECT average_player_age, team_budget, average_attendance FROM teams_features_1 JOIN attendances_features_1 JOIN players_features_1 ON teams_features_1.`team_id`=attendances_features_1.`team_id` AND teams_features_1.`team_id`=players_features_1.`team_id`\n",
      "+------------------+-----------+------------------+\n",
      "|average_player_age|team_budget|average_attendance|\n",
      "+------------------+-----------+------------------+\n",
      "|             25.65|  16758.066|          3271.934|\n",
      "|              25.5|  3966.3591|         4074.8047|\n",
      "|             24.85|    7307.94|         19595.973|\n",
      "|             25.45|   7326.092|          6462.462|\n",
      "|              25.4|   3555.235|          7226.672|\n",
      "+------------------+-----------+------------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "featurestore.get_features(\n",
    "    [\"team_budget\", \"average_attendance\", \"average_player_age\"],\n",
    "    featurestore=featurestore.project_featurestore(),\n",
    "    featuregroups_version_dict={\n",
    "        \"teams_features\": 1, \n",
    "        \"attendances_features\": 1,\n",
    "        \"players_features\": 1\n",
    "    },\n",
    "    join_key = \"team_id\",\n",
    "    dataframe_type = \"spark\"\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advanced Eamples of Fetching Sets of Features and Common Pitfalls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting 12 features from 4 different feature groups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sql: use demo_featurestore_admin000_featurestore\n",
      "Running sql: SELECT team_budget, average_position, sum_player_rating, average_attendance, average_player_worth, sum_player_worth, sum_position, sum_attendance, average_player_rating, team_position, sum_player_age, average_player_age FROM teams_features_1 JOIN season_scores_features_1 JOIN players_features_1 JOIN attendances_features_1 ON teams_features_1.`team_id`=season_scores_features_1.`team_id` AND teams_features_1.`team_id`=players_features_1.`team_id` AND teams_features_1.`team_id`=attendances_features_1.`team_id`\n",
      "+-----------+----------------+-----------------+------------------+--------------------+----------------+------------+--------------+---------------------+-------------+--------------+------------------+\n",
      "|team_budget|average_position|sum_player_rating|average_attendance|average_player_worth|sum_player_worth|sum_position|sum_attendance|average_player_rating|team_position|sum_player_age|average_player_age|\n",
      "+-----------+----------------+-----------------+------------------+--------------------+----------------+------------+--------------+---------------------+-------------+--------------+------------------+\n",
      "|  16758.066|           55.15|        32269.797|          3271.934|           307.87268|       30787.268|      1103.0|      65438.68|            322.69797|           26|        2565.0|             25.65|\n",
      "|  3966.3591|            57.1|        29779.197|         4074.8047|           298.78235|       29878.234|      1142.0|      81496.09|            297.79196|           27|        2550.0|              25.5|\n",
      "|  12474.419|           34.35|         88129.83|          9405.213|           888.29443|       88829.445|       687.0|     188104.27|             881.2983|            9|        2578.0|             25.78|\n",
      "|  1621.1936|            40.3|         46779.38|          7118.376|           490.94702|       49094.703|       806.0|     142367.52|             467.7938|           17|        2601.0|             26.01|\n",
      "|    7307.94|           28.15|        131123.84|         19595.973|           1435.2465|       143524.64|       563.0|     391919.47|            1311.2384|            6|        2485.0|             24.85|\n",
      "+-----------+----------------+-----------------+------------------+--------------------+----------------+------------+--------------+---------------------+-------------+--------------+------------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "featurestore.get_features(\n",
    "    [\"team_budget\", \"average_attendance\", \"average_player_age\",\n",
    "    \"team_position\", \"sum_attendance\", \n",
    "     \"average_player_rating\", \"average_player_worth\", \"sum_player_age\",\n",
    "     \"sum_player_rating\", \"sum_player_worth\", \"sum_position\", \n",
    "     \"average_position\"\n",
    "    ]\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example Errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at an example of a common error that can occur when you query the feature store.\n",
    "\n",
    "If you try to query the feature store for a feature that exists in multiple feature groups, it is impossible for the query planner to infer from which feature group to fetch the feature so it will throw an exception. When this error happen you should specify which feature group to fetch from so that the query planner knows how to get the feature.\n",
    "\n",
    "**Note**: <font color='red'>This cell should fail, don't panic :)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found the feature with name 'team_id' in more than one of the featuregroups of the featurestore: 'demo_featurestore_admin000_featurestore', please specify the optional argument 'featuregroup=', the matched featuregroups were: season_scores_features_1,attendances_features_1,players_features_1,teams_features_1\n",
      "Traceback (most recent call last):\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/python36/lib/python3.6/site-packages/hops/featurestore.py\", line 706, in get_features\n",
      "    return _do_get_features(features, _get_featurestore_metadata(featurestore, update_cache=True), featurestore=featurestore, featuregroups_version_dict=featuregroups_version_dict, join_key=join_key, dataframe_type=dataframe_type)\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/python36/lib/python3.6/site-packages/hops/featurestore.py\", line 783, in _do_get_features\n",
      "    featuregroup_matched = _find_feature(feature, featurestore, featuregroups_parsed)\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/python36/lib/python3.6/site-packages/hops/featurestore.py\", line 446, in _find_feature\n",
      "    featuregroups_matched_str))\n",
      "AssertionError: Found the feature with name 'team_id' in more than one of the featuregroups of the featurestore: 'demo_featurestore_admin000_featurestore', please specify the optional argument 'featuregroup=', the matched featuregroups were: season_scores_features_1,attendances_features_1,players_features_1,teams_features_1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "featurestore.get_features(\n",
    "    [\"team_budget\", \"team_id\"]\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fix the error: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sql: use demo_featurestore_admin000_featurestore\n",
      "Running sql: SELECT team_budget, team_id FROM teams_features_1\n",
      "+-----------+-------+\n",
      "|team_budget|team_id|\n",
      "+-----------+-------+\n",
      "|  12957.076|      1|\n",
      "|  2403.3704|      2|\n",
      "|  3390.3755|      3|\n",
      "|  13547.429|      4|\n",
      "|   9678.333|      5|\n",
      "+-----------+-------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "featurestore.get_features(\n",
    "    [\"team_budget\", \"team_id\"],\n",
    "    featuregroups_version_dict = {\n",
    "        \"teams_features\" : 1\n",
    "    }\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another common error is that you try to fetch features from feature groups that are not compatible, they do not got any natural join column. Typically in this case you need to either provide the join key your self or use SQL directly with `featurestore.sql()`.\n",
    "\n",
    "\n",
    "**Note**: <font color='red'>This cell should fail, don't panic :)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find any common columns in featuregroups to join on, searched through featuregroups: teams_features, games_features\n",
      "Traceback (most recent call last):\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/python36/lib/python3.6/site-packages/hops/featurestore.py\", line 706, in get_features\n",
      "    return _do_get_features(features, _get_featurestore_metadata(featurestore, update_cache=True), featurestore=featurestore, featuregroups_version_dict=featuregroups_version_dict, join_key=join_key, dataframe_type=dataframe_type)\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/python36/lib/python3.6/site-packages/hops/featurestore.py\", line 796, in _do_get_features\n",
      "    join_col = _get_join_col(feature_featuregroups)\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/python36/lib/python3.6/site-packages/hops/featurestore.py\", line 652, in _get_join_col\n",
      "    \"{}\".format(featuregroups_str))\n",
      "AssertionError: Could not find any common columns in featuregroups to join on, searched through featuregroups: teams_features, games_features\n",
      "\n"
     ]
    }
   ],
   "source": [
    "featurestore.get_features(\n",
    "    [\"team_budget\", \"score\"]\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets fix the error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sql: use demo_featurestore_admin000_featurestore\n",
      "Running sql: SELECT team_budget, score FROM teams_features_1 JOIN games_features_1 ON games_features_1.home_team_id = teams_features_1.team_id\n",
      "+-----------+-----+\n",
      "|team_budget|score|\n",
      "+-----------+-----+\n",
      "|  11296.577|    1|\n",
      "|   4969.735|    3|\n",
      "|  21319.533|    2|\n",
      "|  15072.062|    1|\n",
      "|  12957.076|    3|\n",
      "+-----------+-----+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "featurestore.sql(\n",
    "    \"SELECT team_budget, score \" \\\n",
    "    \"FROM teams_features_1 JOIN games_features_1 ON \" \\\n",
    "    \"games_features_1.home_team_id = teams_features_1.team_id\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Free Text SQL Query from the Feature Store\n",
    "\n",
    "For complex queries that cannot be inferred by the helper functions, enter the sql directly to the method `featurestore.sql()` it will default to the project specific feature store but you can also specify it explicitly. If you are proficient in SQL, this is the most efficient and preferred way to query the feature store."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without specifying the feature store the query will by default be run against the project's feature store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sql: use demo_featurestore_admin000_featurestore\n",
      "Running sql: SELECT * FROM teams_features_1 WHERE team_position < 5\n",
      "+-----------+-------+-------------+\n",
      "|team_budget|team_id|team_position|\n",
      "+-----------+-------+-------------+\n",
      "|  12957.076|      1|            1|\n",
      "|  2403.3704|      2|            2|\n",
      "|  3390.3755|      3|            3|\n",
      "|  13547.429|      4|            4|\n",
      "+-----------+-------+-------------+"
     ]
    }
   ],
   "source": [
    "featurestore.sql(\"SELECT * FROM teams_features_1 WHERE team_position < 5\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also specify the featurestore to query and the return format explicitly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sql: use demo_featurestore_admin000_featurestore\n",
      "Running sql: SELECT * FROM teams_features_1 WHERE team_position < 5\n",
      "+-----------+-------+-------------+\n",
      "|team_budget|team_id|team_position|\n",
      "+-----------+-------+-------------+\n",
      "|  12957.076|      1|            1|\n",
      "|  2403.3704|      2|            2|\n",
      "|  3390.3755|      3|            3|\n",
      "|  13547.429|      4|            4|\n",
      "+-----------+-------+-------------+"
     ]
    }
   ],
   "source": [
    "featurestore.sql(\"SELECT * FROM teams_features_1 WHERE team_position < 5\",\n",
    "                featurestore=featurestore.project_featurestore(), \n",
    "                 dataframe_type = \"spark\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing to the Feature Store\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating New Feature Groups\n",
    "\n",
    "In most cases it is recommended that feature groups are created in the UI on Hopsworks and that care is taken in documenting the feature group. \n",
    "\n",
    "![Create Feature Group from the UI](./images/create_fg.png \"Create Feature Group from the UI\")\n",
    "\n",
    "![Create Feature Group from the UI](./images/create_fg_2.png \"Create Feature Group from the UI\")\n",
    "\n",
    "However, sometimes it is practical to create a feature group directly from a spark dataframe and fill in the metadata about the featuregroup later in the UI. This can be done through the `create_featuregroup()` API function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create a new featuregroup called **teams_features_spanish** that contains the same contents as the feature group teams_features except the the columns are renamed to spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sql: use demo_featurestore_admin000_featurestore\n",
      "Running sql: SELECT * FROM teams_features_1"
     ]
    }
   ],
   "source": [
    "teams_features_1_df = featurestore.get_featuregroup(\"teams_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams_features_2_df = teams_features_1_df.withColumnRenamed(\n",
    "    \"team_id\", \"equipo_id\").withColumnRenamed(\n",
    "    \"team_budget\", \"equipo_presupuesto\").withColumnRenamed(\n",
    "    \"team_position\", \"equipo_posicion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------+---------------+\n",
      "|equipo_presupuesto|equipo_id|equipo_posicion|\n",
      "+------------------+---------+---------------+\n",
      "|         12957.076|        1|              1|\n",
      "|         2403.3704|        2|              2|\n",
      "|         3390.3755|        3|              3|\n",
      "|         13547.429|        4|              4|\n",
      "|          9678.333|        5|              5|\n",
      "+------------------+---------+---------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "teams_features_2_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now create a new featuregroup using the transformed dataframe (we'll explain the statistics part later on in this notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sql: use demo_featurestore_admin000_featurestore"
     ]
    }
   ],
   "source": [
    "featurestore.create_featuregroup(\n",
    "    teams_features_2_df,\n",
    "    \"teams_features_spanish\",\n",
    "    description=\"a spanish version of teams_features\",\n",
    "    descriptive_statistics=False,\n",
    "    feature_correlation=False,\n",
    "    feature_histograms=False,\n",
    "    cluster_analysis=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default the new featuregroup will be created in the project's featurestore and the statistics for the new featuregroup will be computed based on the provided spark dataframe. You can configure this behaviour by modifying the default arguments and filling in extra metadata.\n",
    "\n",
    "The dependencies argument takes a list of HDFS file names that the feature group depends on, i.e when the datasets that a featuregroup depends on have been modified, the feature group should be recomputed. The dependencies can also be updated and viewed in the feature registry UI. \n",
    "\n",
    "![Feature group dependencies](./images/deps.png \"Feature group dependencies\")\n",
    "\n",
    "![Feature group dependencies](./images/deps2.png \"Feature group dependencies\")\n",
    "\n",
    "The jobId argument takes an integer that identifies the job id to compute the features. Once you have created a job that creates/inserts features in the feature store you can use the Featurestore UI to link that job to the featuregroup:\n",
    "\n",
    "![Feature group jobs](./images/jobs1.png \"Feature group jobs\")\n",
    "\n",
    "![Feature group jobs](./images/jobs2.png \"Feature group jobs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sql: use demo_featurestore_admin000_featurestore"
     ]
    }
   ],
   "source": [
    "featurestore.create_featuregroup(\n",
    "    teams_features_2_df,\n",
    "    \"teams_features_spanish\",\n",
    "    description=\"a spanish version of teams_features\",\n",
    "    descriptive_statistics=False,\n",
    "    feature_correlation=False,\n",
    "    feature_histograms=False,\n",
    "    cluster_analysis=False,\n",
    "    featurestore=featurestore.project_featurestore(),\n",
    "    featuregroup_version=1,\n",
    "    job_name=None,\n",
    "    dependencies=[]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Create a New Version of A Feature Group\n",
    "\n",
    "To create a new version, simply use the `create_featuregroup` method and specify the version argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sql: use demo_featurestore_admin000_featurestore"
     ]
    }
   ],
   "source": [
    "featurestore.create_featuregroup(\n",
    "    teams_features_2_df,\n",
    "    \"teams_features_spanish\",\n",
    "    description=\"a spanish version of teams_features\",\n",
    "    descriptive_statistics=False,\n",
    "    feature_correlation=False,\n",
    "    feature_histograms=False,\n",
    "    cluster_analysis=False,\n",
    "    featuregroup_version=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now see the new version in the feature store UI:\n",
    "\n",
    "![Create Feature Group Version](./images/create_fg_version.png \"Create Feature Group Version\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the Latest Version of a Feature Group (0 if no version exist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2"
     ]
    }
   ],
   "source": [
    "latest_version = featurestore.get_latest_featuregroup_version(\"teams_features_spanish\")\n",
    "latest_version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update Metadata Cache\n",
    "\n",
    "Note: By default, the python client will only fetch the featurestore metadata once and then cache it on the client. If you need to update the cache you can use the command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore.get_featurestore_metadata(update_cache=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Featuregroup Partitioning \n",
    "\n",
    "Featuregroups are stored as Hive tables, meaning that they can be partitioned for improved **read-query** performance. We use dynamic partitioning where the partition keys are specified on creation of a featuregroup. To set the partitions, simply specify the `partition_by` argument to `create_featuregroup()`. The `partition_by` argument should be set as a python list of the columns that you want to partition the table on, see examples below.\n",
    "\n",
    "Partitioning is not supported for training datasets as those are meant to be immutable blobs used for training (e.g petastorm or tfrecords), and do not need to be optimized for query performance. \n",
    "\n",
    "Feature groups in Hive might need to be read-query optimized however, since it might be used to create a lot of different training datasets, using different subsets of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example we can take the `games_features` featuregroup created by the featurestore tour and re-create it as a new featuregroup called `games_features_partitioned` where we partition on the `score` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sql: use demo_featurestore_admin000_featurestore\n",
      "Running sql: SELECT * FROM games_features_1"
     ]
    }
   ],
   "source": [
    "games_features_df = featurestore.get_featuregroup(\"games_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sql: use demo_featurestore_admin000_featurestore\n",
      "Feature group created successfully"
     ]
    }
   ],
   "source": [
    "featurestore.create_featuregroup(\n",
    "    games_features_df,\n",
    "    \"games_features_partitioned\",\n",
    "    description=\"games_features partitioned by score\",\n",
    "    descriptive_statistics=False,\n",
    "    feature_correlation=False,\n",
    "    feature_histograms=False,\n",
    "    cluster_analysis=False,\n",
    "    partition_by=[\"score\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also partition on multiple columns (watch out so that the number of partitions don't get too many though). Also note that it is not allowed to partition on the primary key, which does not make sense either since the primary key should be unique. If you try to partition on the primary key, the partitioning will simply be skipped. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sql: use demo_featurestore_admin000_featurestore\n",
      "Feature group created successfully"
     ]
    }
   ],
   "source": [
    "featurestore.create_featuregroup(\n",
    "    games_features_df,\n",
    "    \"games_features_double_partitioned\",\n",
    "    description=\"games_features partitioned by score and away_team_id\",\n",
    "    descriptive_statistics=False,\n",
    "    feature_correlation=False,\n",
    "    feature_histograms=False,\n",
    "    cluster_analysis=False,\n",
    "    partition_by=[\"score\", \"home_team_id\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify the partitions we can use the utility function `get_featuregroup_partitions`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sql: use demo_featurestore_admin000_featurestore\n",
      "Running sql: SHOW PARTITIONS games_features_partitioned_1\n",
      "+---------+\n",
      "|partition|\n",
      "+---------+\n",
      "|  score=1|\n",
      "|  score=2|\n",
      "|  score=3|\n",
      "+---------+"
     ]
    }
   ],
   "source": [
    "featurestore.get_featuregroup_partitions(\"games_features_partitioned\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sql: use demo_featurestore_admin000_featurestore\n",
      "Running sql: SHOW PARTITIONS games_features_double_partitioned_1\n",
      "+--------------------+\n",
      "|           partition|\n",
      "+--------------------+\n",
      "|home_team_id=1/sc...|\n",
      "|home_team_id=1/sc...|\n",
      "|home_team_id=10/s...|\n",
      "|home_team_id=10/s...|\n",
      "|home_team_id=10/s...|\n",
      "|home_team_id=11/s...|\n",
      "|home_team_id=11/s...|\n",
      "|home_team_id=11/s...|\n",
      "|home_team_id=12/s...|\n",
      "|home_team_id=12/s...|\n",
      "+--------------------+\n",
      "only showing top 10 rows"
     ]
    }
   ],
   "source": [
    "featurestore.get_featuregroup_partitions(\"games_features_double_partitioned\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also specify the optional arguments for `get_featuregroup_partitions`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sql: use demo_featurestore_admin000_featurestore\n",
      "Running sql: SHOW PARTITIONS games_features_partitioned_1\n",
      "+---------+\n",
      "|partition|\n",
      "+---------+\n",
      "|  score=1|\n",
      "|  score=2|\n",
      "|  score=3|\n",
      "+---------+"
     ]
    }
   ],
   "source": [
    "featurestore.get_featuregroup_partitions(\"games_features_partitioned\",\n",
    "                                         featurestore=featurestore.project_featurestore(),\n",
    "                                         featuregroup_version = 1,\n",
    "                                         dataframe_type=\"spark\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you overwrite a table, it will pick up the old partitioning scheme it had before the overwrite: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing descriptive statistics for : games_features_partitioned\n",
      "computing feature correlation for: games_features_partitioned\n",
      "computing feature histograms for: games_features_partitioned\n",
      "computing cluster analysis for: games_features_partitioned\n",
      "Running sql: use demo_featurestore_admin000_featurestore"
     ]
    }
   ],
   "source": [
    "featurestore.insert_into_featuregroup(games_features_df, \"games_features_partitioned\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sql: use demo_featurestore_admin000_featurestore\n",
      "Running sql: SHOW PARTITIONS games_features_partitioned_1\n",
      "+---------+\n",
      "|partition|\n",
      "+---------+\n",
      "|  score=1|\n",
      "|  score=2|\n",
      "|  score=3|\n",
      "+---------+"
     ]
    }
   ],
   "source": [
    "featurestore.get_featuregroup_partitions(\"games_features_partitioned\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inserting Into Existing Feature Groups\n",
    "\n",
    "A best practice when working with features in HopsML is to first figure out a model of feature groups and create them  using the Feature Registry UI. This will prepare the feature group schema and create the Hive tables. Once the empty feature groups are created, then you can insert into these tables directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets first get some sample data to insert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, LongType, IntegerType, FloatType\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([StructField(\"equipo_id\", IntegerType(), True),\n",
    "                     StructField(\"equipo_presupuesto\", FloatType(), True),\n",
    "                     StructField(\"equipo_posicion\", IntegerType(), True)\n",
    "                        ])\n",
    "sample_df = sqlContext.createDataFrame([(999, 41251.52, 1), (998, 1319.4, 8), (997, 21219.1, 2)], schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+---------------+\n",
      "|equipo_id|equipo_presupuesto|equipo_posicion|\n",
      "+---------+------------------+---------------+\n",
      "|      999|          41251.52|              1|\n",
      "|      998|            1319.4|              8|\n",
      "|      997|           21219.1|              2|\n",
      "+---------+------------------+---------------+"
     ]
    }
   ],
   "source": [
    "sample_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3"
     ]
    }
   ],
   "source": [
    "sample_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets inspect the contents of the featuregroup `teams_features_spanish` that we are going to insert the sample data into:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sql: use demo_featurestore_admin000_featurestore\n",
      "Running sql: SELECT * FROM teams_features_spanish_1"
     ]
    }
   ],
   "source": [
    "spanish_team_features_df = featurestore.get_featuregroup(\n",
    "    \"teams_features_spanish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------+---------------+\n",
      "|equipo_presupuesto|equipo_id|equipo_posicion|\n",
      "+------------------+---------+---------------+\n",
      "|         12957.076|        1|              1|\n",
      "|         2403.3704|        2|              2|\n",
      "|         3390.3755|        3|              3|\n",
      "|         13547.429|        4|              4|\n",
      "|          9678.333|        5|              5|\n",
      "+------------------+---------+---------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "spanish_team_features_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50"
     ]
    }
   ],
   "source": [
    "spanish_team_features_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can insert the sample data and verify the new contents of the featuregroup. By default the insert mode is \"append\", the featurestore is the project's featurestore, the version is 1 and statistics will be updated (we cover statistics later on in this notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sql: use demo_featurestore_admin000_featurestore"
     ]
    }
   ],
   "source": [
    "featurestore.insert_into_featuregroup(\n",
    "    sample_df, \n",
    "    \"teams_features_spanish\", \n",
    "    descriptive_statistics=False, \n",
    "    feature_correlation=False,\n",
    "    feature_histograms=False,\n",
    "    cluster_analysis=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now inspect the contents of the feature group to verify that the insertion was successful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sql: use demo_featurestore_admin000_featurestore\n",
      "Running sql: SELECT * FROM teams_features_spanish_1"
     ]
    }
   ],
   "source": [
    "spanish_team_features_df_updated = featurestore.get_featuregroup(\n",
    "    \"teams_features_spanish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------+---------------+\n",
      "|equipo_presupuesto|equipo_id|equipo_posicion|\n",
      "+------------------+---------+---------------+\n",
      "|         12957.076|        1|              1|\n",
      "|         2403.3704|        2|              2|\n",
      "|         3390.3755|        3|              3|\n",
      "|         13547.429|        4|              4|\n",
      "|          9678.333|        5|              5|\n",
      "+------------------+---------+---------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "spanish_team_features_df_updated.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53"
     ]
    }
   ],
   "source": [
    "spanish_team_features_df_updated.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also explicitly specify featurestore, featuregroup version, the insert mode and what statistics to compute (if any):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sql: use demo_featurestore_admin000_featurestore"
     ]
    }
   ],
   "source": [
    "featurestore.insert_into_featuregroup(\n",
    "    sample_df, \n",
    "    \"teams_features_spanish\", \n",
    "    featurestore=featurestore.project_featurestore(), \n",
    "    featuregroup_version=1, \n",
    "    mode=\"append\",\n",
    "    descriptive_statistics=False, \n",
    "    feature_correlation=False, \n",
    "    feature_histograms=False,\n",
    "    cluster_analysis=False, \n",
    "    stat_columns=None, \n",
    "    num_bins=20, \n",
    "    corr_method='pearson',\n",
    "    num_clusters=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sql: use demo_featurestore_admin000_featurestore\n",
      "Running sql: SELECT * FROM teams_features_spanish_1\n",
      "56"
     ]
    }
   ],
   "source": [
    "featurestore.get_featuregroup(\"teams_features_spanish\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two supported insert modes are \"append\" and \"overwrite\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sql: use demo_featurestore_admin000_featurestore"
     ]
    }
   ],
   "source": [
    "featurestore.insert_into_featuregroup(\n",
    "    sample_df, \n",
    "    \"teams_features_spanish\",\n",
    "    descriptive_statistics=False, \n",
    "    feature_correlation=False,\n",
    "    feature_histograms=False,\n",
    "    cluster_analysis=False,\n",
    "    mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sql: use demo_featurestore_admin000_featurestore\n",
      "Running sql: SELECT * FROM teams_features_spanish_1\n",
      "+---------+---------------+------------------+\n",
      "|equipo_id|equipo_posicion|equipo_presupuesto|\n",
      "+---------+---------------+------------------+\n",
      "|      998|              8|            1319.4|\n",
      "|      997|              2|           21219.1|\n",
      "|      999|              1|          41251.52|\n",
      "+---------+---------------+------------------+"
     ]
    }
   ],
   "source": [
    "featurestore.get_featuregroup(\"teams_features_spanish\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sql: use demo_featurestore_admin000_featurestore\n",
      "Running sql: SELECT * FROM teams_features_spanish_1\n",
      "3"
     ]
    }
   ],
   "source": [
    "featurestore.get_featuregroup(\"teams_features_spanish\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrating the Feature Store with Pandas and Numpy\n",
    "\n",
    "The Hops Feature Store works natively with Spark, but you can easily connect it to your numpy/pandas/pure python pipelines as well (just recall that if you are working with big data, using libraries like numpy/pandas that are not distributed will not scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Features from the Featue Store into a Pandas or Numpy Table\n",
    "\n",
    "To read from the feature store into pandas and numpy we just have to specify the optional argument `dataframe_type` that defaults to \"spark\". This argument decides the format of the returning dataframe. If you want the features returned to be in either pandas or numpy format you can specfy `dataframe_type='pandas'` or `dataframe_type='numpy'`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading Features into a Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sql: use demo_featurestore_admin000_featurestore\n",
      "Running sql: SELECT average_player_age, team_budget, average_attendance FROM players_features_1 JOIN teams_features_1 JOIN attendances_features_1 ON players_features_1.`team_id`=teams_features_1.`team_id` AND players_features_1.`team_id`=attendances_features_1.`team_id`"
     ]
    }
   ],
   "source": [
    "pandas_df = featurestore.get_features([\"team_budget\", \"average_attendance\", \"average_player_age\"], \n",
    "                                      dataframe_type=\"pandas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   average_player_age   team_budget  average_attendance\n",
      "0           24.850000   7307.939941        19595.972656\n",
      "1           25.450001   7326.091797         6462.461914\n",
      "2           25.400000   3555.235107         7226.671875\n",
      "3           25.910000    910.393250         3189.845459\n",
      "4           25.780001  12474.418945         9405.212891"
     ]
    }
   ],
   "source": [
    "pandas_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading Features into a Numpy 2D Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sql: use demo_featurestore_admin000_featurestore\n",
      "Running sql: SELECT average_player_age, team_budget, average_attendance FROM players_features_1 JOIN teams_features_1 JOIN attendances_features_1 ON players_features_1.`team_id`=teams_features_1.`team_id` AND players_features_1.`team_id`=attendances_features_1.`team_id`"
     ]
    }
   ],
   "source": [
    "numpy_df = featurestore.get_features([\"team_budget\", \"average_attendance\", \"average_player_age\"], \n",
    "                                      dataframe_type=\"numpy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 3)"
     ]
    }
   ],
   "source": [
    "numpy_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading Features into a Python 2D List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sql: use demo_featurestore_admin000_featurestore\n",
      "Running sql: SELECT average_player_age, team_budget, average_attendance FROM players_features_1 JOIN teams_features_1 JOIN attendances_features_1 ON players_features_1.`team_id`=teams_features_1.`team_id` AND players_features_1.`team_id`=attendances_features_1.`team_id`"
     ]
    }
   ],
   "source": [
    "python_df = featurestore.get_features([\"team_budget\", \"average_attendance\", \"average_player_age\"], \n",
    "                                      dataframe_type=\"python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50"
     ]
    }
   ],
   "source": [
    "len(python_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3"
     ]
    }
   ],
   "source": [
    "len(python_df[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Pandas or Numpy Tables into the Feature Store\n",
    "\n",
    "The feature store API natively supports dataframes in spark, pandas, python or numpy format for writing to the feature store. \n",
    "\n",
    "**Note** that since the feature store contains feature groups with documented schemas to allow easy joining of thousands of features in the future, inserting raw numpy arrays is not recommended. Numpy arrays and python lists do not have any associated schema so the library will infer the schema to be \"col_1, col_2, col_3... etc\". It is recommended that you think carefully in how to model your feature schema with naming of columns **before** you insert into the feature store so that it becomed easy to later join features across several feature groups by using a common join-column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing a Pandas DataFrame to the Feature Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['average_player_age', 'team_budget', 'average_attendance'], dtype='object')"
     ]
    }
   ],
   "source": [
    "pandas_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's rename the columns to differentiate this feature group from existing ones in the feature store\n",
    "pandas_df.columns = [\"average_player_age_test\", \"team_budget_test\", \"average_attendance_test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sql: use demo_featurestore_admin000_featurestore"
     ]
    }
   ],
   "source": [
    "featurestore.create_featuregroup(\n",
    "    pandas_df,\n",
    "    \"pandas_test_example\",\n",
    "    description=\"test featuregroup created from pandas dataframe\",\n",
    "    descriptive_statistics=False,\n",
    "    feature_correlation=False,\n",
    "    feature_histograms=False,\n",
    "    cluster_analysis=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sql: use demo_featurestore_admin000_featurestore"
     ]
    }
   ],
   "source": [
    "# All insert/create methods in the API supports spark, pandas, and numpy dataframes\n",
    "featurestore.insert_into_featuregroup(\n",
    "    pandas_df, \n",
    "    \"pandas_test_example\",\n",
    "    descriptive_statistics=False, \n",
    "    feature_correlation=False,\n",
    "    feature_histograms=False,\n",
    "    cluster_analysis=False,\n",
    "    mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing a Numpy 2D Array to the Feature Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sql: use demo_featurestore_admin000_featurestore"
     ]
    }
   ],
   "source": [
    "featurestore.create_featuregroup(\n",
    "    numpy_df,\n",
    "    \"numpy_test_example\",\n",
    "    description=\"test featuregroup created from numpy matrix\",\n",
    "    descriptive_statistics=False,\n",
    "    feature_correlation=False,\n",
    "    feature_histograms=False,\n",
    "    cluster_analysis=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As Numpy arrays do not have an associated schema the resulting feature group will have default column names. A best practice if you are working with numpy is to convert it to a pandas or spark dataframe and specify an explicit schema before you save it to the feature store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sql: use demo_featurestore_admin000_featurestore\n",
      "Running sql: SELECT * FROM numpy_test_example_1\n",
      "+------------------+-----------------+-----------------+\n",
      "|             col_0|            col_1|            col_2|\n",
      "+------------------+-----------------+-----------------+\n",
      "|24.850000381469727| 7307.93994140625|   19595.97265625|\n",
      "|25.450000762939453|   7326.091796875|  6462.4619140625|\n",
      "|25.399999618530273|3555.235107421875|      7226.671875|\n",
      "| 25.90999984741211|910.3932495117188|3189.845458984375|\n",
      "|25.780000686645508| 12474.4189453125|   9405.212890625|\n",
      "+------------------+-----------------+-----------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "featurestore.get_featuregroup(\"numpy_test_example\", dataframe_type=\"spark\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sql: use demo_featurestore_admin000_featurestore"
     ]
    }
   ],
   "source": [
    "# All insert/create methods in the API supports spark, pandas, and numpy dataframes\n",
    "featurestore.insert_into_featuregroup(\n",
    "    numpy_df, \n",
    "    \"numpy_test_example\",\n",
    "    descriptive_statistics=False, \n",
    "    feature_correlation=False,\n",
    "    feature_histograms=False,\n",
    "    cluster_analysis=False,\n",
    "    mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing a Python 2D List to the Feature Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sql: use demo_featurestore_admin000_featurestore"
     ]
    }
   ],
   "source": [
    "featurestore.create_featuregroup(\n",
    "    python_df,\n",
    "    \"python_test_example\",\n",
    "    description=\"test featuregroup created from python 2D list\",\n",
    "    descriptive_statistics=False,\n",
    "    feature_correlation=False,\n",
    "    feature_histograms=False,\n",
    "    cluster_analysis=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sql: use demo_featurestore_admin000_featurestore"
     ]
    }
   ],
   "source": [
    "# All insert/create methods in the API supports spark, pandas, and numpy dataframes\n",
    "featurestore.insert_into_featuregroup(\n",
    "    python_df, \n",
    "    \"python_test_example\",\n",
    "    descriptive_statistics=False, \n",
    "    feature_correlation=False,\n",
    "    feature_histograms=False,\n",
    "    cluster_analysis=False,\n",
    "    mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As python lists do not have an associated schema the resulting feature group will have default column names. A best practice if you are working with python lists is to convert it to a pandas or spark dataframe and specify an explicit schema before you save it to the feature store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sql: use demo_featurestore_admin000_featurestore\n",
      "Running sql: SELECT * FROM python_test_example_1\n",
      "+------------------+-----------------+-----------------+\n",
      "|             col_0|            col_1|            col_2|\n",
      "+------------------+-----------------+-----------------+\n",
      "|24.850000381469727| 7307.93994140625|   19595.97265625|\n",
      "|25.450000762939453|   7326.091796875|  6462.4619140625|\n",
      "|25.399999618530273|3555.235107421875|      7226.671875|\n",
      "| 25.90999984741211|910.3932495117188|3189.845458984375|\n",
      "|25.780000686645508| 12474.4189453125|   9405.212890625|\n",
      "+------------------+-----------------+-----------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "featurestore.get_featuregroup(\"python_test_example\", dataframe_type=\"spark\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Errors\n",
    "\n",
    "To insert numpy/python arrays you must make sure that the dimensions match, i.e it needs to be in two dimensions\n",
    "\n",
    "**Note**: <font color='red'>This cell should fail, don't panic :)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not convert the provided dataframe to a spark dataframe which is required in order to save it to the Feature Store, error: Cannot convert numpy array that do not have two dimensions to a dataframe. The number of dimensions are: 1\n",
      "Traceback (most recent call last):\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/python36/lib/python3.6/site-packages/hops/featurestore.py\", line 1779, in create_featuregroup\n",
      "    str(e)))\n",
      "AssertionError: Could not convert the provided dataframe to a spark dataframe which is required in order to save it to the Feature Store, error: Cannot convert numpy array that do not have two dimensions to a dataframe. The number of dimensions are: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "python_fail_test = [1,2,3]\n",
    "featurestore.create_featuregroup(\n",
    "    python_fail_test,\n",
    "    \"python_fail_test\",\n",
    "    description=\"\",\n",
    "    descriptive_statistics=False,\n",
    "    feature_correlation=False,\n",
    "    feature_histograms=False,\n",
    "    cluster_analysis=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Group Statistics\n",
    "\n",
    "Statistics about a featuregroup can be useful in the stage of feature engineering and when deciding which features to use for training. If statistics have been computed for a feature group, it can be viewed in the Hopsworks Feature Registry UI. \n",
    "\n",
    "This is particularly useful within large organizations where data scientists from different teams can re-use and explore new features by browsing features in the feature store and analyzing the statistics.\n",
    "\n",
    "![Feature Registry Statistics Visualization](./images/fg_stats_1.png \"Feature Registry Statistics Visualization\")\n",
    "\n",
    "![Feature Registry Statistics Visualization](./images/fg_stats_2.png \"Feature Registry Statistics Visualization\")\n",
    "\n",
    "![Feature Registry Statistics Visualization](./images/fg_stats_3.png \"Feature Registry Statistics Visualization\")\n",
    "\n",
    "![Feature Registry Statistics Visualization](./images/fg_stats_4.png \"Feature Registry Statistics Visualization\")\n",
    "\n",
    "![Feature Registry Statistics Visualization](./images/fg_stats_5.png \"Feature Registry Statistics Visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you might have notived earlier in this notebook, both the `insert_into_featuregroup` and `create_featuregroup` methods have arguments for updating the statistics as new data is added. \n",
    "\n",
    "You can also use the `update_featuregroup_stats()` method to update the statistics of a feature group without inserting any new data. By default it will compute all statistics (descriptive, feature correlation, histograms, and cluster analysis), use the project's featurestore, use version 1 of the featuregroup and use all columns for computing statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sql: use demo_featurestore_admin000_featurestore\n",
      "Running sql: SELECT * FROM teams_features_1\n",
      "computing descriptive statistics for : teams_features\n",
      "computing feature correlation for: teams_features\n",
      "computing feature histograms for: teams_features\n",
      "computing cluster analysis for: teams_features"
     ]
    }
   ],
   "source": [
    "featurestore.update_featuregroup_stats(\"teams_features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also be explicitly specify featuregroup details and what statistics to compute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sql: use demo_featurestore_admin000_featurestore\n",
      "Running sql: SELECT * FROM teams_features_1\n",
      "computing descriptive statistics for : teams_features\n",
      "computing feature correlation for: teams_features\n",
      "computing feature histograms for: teams_features\n",
      "computing cluster analysis for: teams_features"
     ]
    }
   ],
   "source": [
    "featurestore.update_featuregroup_stats(\n",
    "    \"teams_features\", \n",
    "    featuregroup_version=1, \n",
    "    featurestore=featurestore.project_featurestore(), \n",
    "    descriptive_statistics=True,\n",
    "    feature_correlation=True, \n",
    "    feature_histograms=True,\n",
    "    cluster_analysis=True,\n",
    "    stat_columns=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you only want to compute statistics for certain set of columns and exclude surrogate key-columns for example, you can use the optional argument `stat_columns` to specify which columns to include:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sql: use demo_featurestore_admin000_featurestore\n",
      "Running sql: SELECT * FROM teams_features_1\n",
      "computing descriptive statistics for : teams_features\n",
      "computing feature correlation for: teams_features\n",
      "computing feature histograms for: teams_features\n",
      "computing cluster analysis for: teams_features"
     ]
    }
   ],
   "source": [
    "featurestore.update_featuregroup_stats(\n",
    "    \"teams_features\", \n",
    "    featuregroup_version=1, \n",
    "    featurestore=featurestore.project_featurestore(), \n",
    "    descriptive_statistics=True,\n",
    "    feature_correlation=True, \n",
    "    feature_histograms=True,\n",
    "    cluster_analysis=True,\n",
    "    stat_columns=['team_budget', 'team_position'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Datasets\n",
    "\n",
    "To group data in the feature store we use three concepts:\n",
    "\n",
    "- Feature\n",
    "- Feature group\n",
    "- Training Dataset\n",
    "\n",
    "Typically during the feature engineering phase of a machine learning project, you compute a set of features for each type of data that you have, these features are naturally grouped into a documented and versioned **feature group**. \n",
    "\n",
    "In practice, it is common that organizations have many different type of datasets that they can extract features from, for example if you are building a recommendation system you might have demographic data about each user as well as user-activity data. \n",
    "\n",
    "When you train a machine learning model, you want to use all features that have predictive power and that the model can learn from. At this point, we can create a training dataset of features from several different feature groups and use that for training. That is the purpose of the training dataset abstraction. \n",
    "\n",
    "Of course you can always just save a group of features anywhere inside your project, e.g as a csv, or .tfrecords file. However, by using the feature store you can create **managed** training datasets. Managed training datasets will show up in the feature registry UI and will automatically be versioned, documented and reproducible. \n",
    "\n",
    "![Feature Engineering Pipeline](./images/pipeline.png \"Feature Engineering Pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metadata for a training dataset can be created from the Hopsworks UI or directly from the API with the function `create_training_dataset`. The training datasets in a project are stored in a top-level dataset called `<ProjectName>_Training_Datasets`, (i.e `hdfs:///Projects/<ProjectName>/<ProjectName>_Training_Datasets`.\n",
    "\n",
    "Once a training dataset have been created you can find it in the featurestore UI in hopsworks under the tab `Training datasets`, from there you can also edit the metadata if necessary. \n",
    "\n",
    "![Find Training Datasets](./images/find_training_datasets.png \"Find Training Datasets\")\n",
    "After a training dataset have been created with the necessary metadata you can save the actual data in the training dataset by using the API function `insert_into_training_dataset`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create New Training Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create a dataset called `team_position_prediction` by using a set of relevant features from the featurestore. We will combine features from four different feature groups to form this training dataset: `teams_features`, `attendances_features`, `players_features`, `season_scores_features`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sql: use demo_featurestore_admin000_featurestore\n",
      "Running sql: SELECT team_budget, average_position, sum_player_rating, average_attendance, average_player_worth, sum_player_worth, sum_position, sum_attendance, average_player_rating, team_position, sum_player_age, average_player_age FROM teams_features_1 JOIN season_scores_features_1 JOIN players_features_1 JOIN attendances_features_1 ON teams_features_1.`team_id`=season_scores_features_1.`team_id` AND teams_features_1.`team_id`=players_features_1.`team_id` AND teams_features_1.`team_id`=attendances_features_1.`team_id`"
     ]
    }
   ],
   "source": [
    "features_df = featurestore.get_features(\n",
    "    [\"team_budget\", \"average_attendance\", \"average_player_age\",\n",
    "    \"team_position\", \"sum_attendance\", \n",
    "     \"average_player_rating\", \"average_player_worth\", \"sum_player_age\",\n",
    "     \"sum_player_rating\", \"sum_player_worth\", \"sum_position\", \n",
    "     \"average_position\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+-----------------+------------------+--------------------+----------------+------------+--------------+---------------------+-------------+--------------+------------------+\n",
      "|team_budget|average_position|sum_player_rating|average_attendance|average_player_worth|sum_player_worth|sum_position|sum_attendance|average_player_rating|team_position|sum_player_age|average_player_age|\n",
      "+-----------+----------------+-----------------+------------------+--------------------+----------------+------------+--------------+---------------------+-------------+--------------+------------------+\n",
      "|  16758.066|           55.15|        32269.797|          3271.934|           307.87268|       30787.268|      1103.0|      65438.68|            322.69797|           26|        2565.0|             25.65|\n",
      "|  3966.3591|            57.1|        29779.197|         4074.8047|           298.78235|       29878.234|      1142.0|      81496.09|            297.79196|           27|        2550.0|              25.5|\n",
      "|  12474.419|           34.35|         88129.83|          9405.213|           888.29443|       88829.445|       687.0|     188104.27|             881.2983|            9|        2578.0|             25.78|\n",
      "|  1621.1936|            40.3|         46779.38|          7118.376|           490.94702|       49094.703|       806.0|     142367.52|             467.7938|           17|        2601.0|             26.01|\n",
      "|    7307.94|           28.15|        131123.84|         19595.973|           1435.2465|       143524.64|       563.0|     391919.47|            1311.2384|            6|        2485.0|             24.85|\n",
      "+-----------+----------------+-----------------+------------------+--------------------+----------------+------------+--------------+---------------------+-------------+--------------+------------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "features_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the Latest Version of a Training Dataset (0 if no version exist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0"
     ]
    }
   ],
   "source": [
    "latest_version = featurestore.get_latest_training_dataset_version(\"team_position_prediction\")\n",
    "latest_version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save a Training Dataset in TFRecords Format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create a training dataset from the dataframe with some extended metadata such as schema (automatically inferred). By default when you create a training dataset it will be in \"tfrecords\" format and statistics will be computed for all features. After the dataset have been created you can view and/or update the metadata about the training dataset from the Hopsworks featurestore UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore.create_training_dataset(\n",
    "    features_df, \"team_position_prediction\",\n",
    "    descriptive_statistics=False,\n",
    "    feature_correlation=False,\n",
    "    feature_histograms=False,\n",
    "    cluster_analysis=False,\n",
    "    training_dataset_version = latest_version + 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we now go to the Feature Registy UI in Hopsworks we can see that the training dataset have been created for us and things like versioning, documentation, and recomputation is managed for us. We can also easily edit the metadata from the UI if necesssary.\n",
    "\n",
    "![Training Dataset UI](./images/training_dataset.png \"Training Dataset UI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can override the default configuration if necessary:\n",
    "\n",
    "Supported data formats are:\n",
    "\n",
    "- csv (written with spark distributed)\n",
    "- tsv (written with spark distributed)\n",
    "- parquet (written with spark distributed)\n",
    "- tfrecords (written with spark distributed)\n",
    "- avro (written with spark distributed)\n",
    "- orc (written with spark distributed)\n",
    "- hdf5 (written with single-machine, must fit into memory)\n",
    "- npy (written with single-machine, must fit into memory)\n",
    "- petastorm (written with spark distributed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save a Training Dataset in  CSV format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore.create_training_dataset(\n",
    "    features_df, \"team_position_prediction_csv\",\n",
    "    description=\"a dataset with features for football teams, used for training a model to predict league-position\",\n",
    "    featurestore=featurestore.project_featurestore(),\n",
    "    data_format=\"csv\",\n",
    "    training_dataset_version=featurestore.get_latest_training_dataset_version(\"team_position_prediction_csv\") + 1,\n",
    "    job_name=None,\n",
    "    dependencies=[],\n",
    "    descriptive_statistics=False,\n",
    "    feature_correlation=False,\n",
    "    feature_histograms=False,\n",
    "    cluster_analysis=False,\n",
    "    stat_columns=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save a Training Dataset in  TSV format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore.create_training_dataset(\n",
    "    features_df, \"team_position_prediction_tsv\",\n",
    "    description=\"a dataset with features for football teams, used for training a model to predict league-position\",\n",
    "    featurestore=featurestore.project_featurestore(),\n",
    "    data_format=\"tsv\",\n",
    "    training_dataset_version=featurestore.get_latest_training_dataset_version(\"team_position_prediction_tsv\") + 1,\n",
    "    job_name=None,\n",
    "    dependencies=[],\n",
    "    descriptive_statistics=False,\n",
    "    feature_correlation=False,\n",
    "    feature_histograms=False,\n",
    "    cluster_analysis=False,\n",
    "    stat_columns=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save a Training Dataset in Parquet format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore.create_training_dataset(\n",
    "    features_df, \"team_position_prediction_parquet\",\n",
    "    description=\"a dataset with features for football teams, used for training a model to predict league-position\",\n",
    "    featurestore=featurestore.project_featurestore(),\n",
    "    data_format=\"parquet\",\n",
    "    training_dataset_version=featurestore.get_latest_training_dataset_version(\"team_position_prediction_parquet\") + 1,\n",
    "    job_name=None,\n",
    "    dependencies=[],\n",
    "    descriptive_statistics=False,\n",
    "    feature_correlation=False,\n",
    "    feature_histograms=False,\n",
    "    cluster_analysis=False,\n",
    "    stat_columns=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save a Training Dataset in ORC format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore.create_training_dataset(\n",
    "    features_df, \"team_position_prediction_orc\",\n",
    "    description=\"a dataset with features for football teams, used for training a model to predict league-position\",\n",
    "    featurestore=featurestore.project_featurestore(),\n",
    "    data_format=\"orc\",\n",
    "    training_dataset_version=featurestore.get_latest_training_dataset_version(\"team_position_prediction_orc\") + 1,\n",
    "    job_name=None,\n",
    "    dependencies=[],\n",
    "    descriptive_statistics=False,\n",
    "    feature_correlation=False,\n",
    "    feature_histograms=False,\n",
    "    cluster_analysis=False,\n",
    "    stat_columns=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save a Training Dataset in avro format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore.create_training_dataset(\n",
    "    features_df, \"team_position_prediction_avro\",\n",
    "    description=\"a dataset with features for football teams, used for training a model to predict league-position\",\n",
    "    featurestore=featurestore.project_featurestore(),\n",
    "    data_format=\"avro\",\n",
    "    training_dataset_version=featurestore.get_latest_training_dataset_version(\"team_position_prediction_avro\") + 1,\n",
    "    job_name=None,\n",
    "    dependencies=[],\n",
    "    descriptive_statistics=False,\n",
    "    feature_correlation=False,\n",
    "    feature_histograms=False,\n",
    "    cluster_analysis=False,\n",
    "    stat_columns=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save a Training Dataset in HDF5 format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore.create_training_dataset(\n",
    "    features_df, \"team_position_prediction_hdf5\",\n",
    "    description=\"a dataset with features for football teams, used for training a model to predict league-position\",\n",
    "    featurestore=featurestore.project_featurestore(),\n",
    "    data_format=\"hdf5\",\n",
    "    training_dataset_version=featurestore.get_latest_training_dataset_version(\"team_position_prediction_hdf5\") + 1,\n",
    "    job_name=None,\n",
    "    dependencies=[],\n",
    "    descriptive_statistics=False,\n",
    "    feature_correlation=False,\n",
    "    feature_histograms=False,\n",
    "    cluster_analysis=False,\n",
    "    stat_columns=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save a Training Dataset in  .npy format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore.create_training_dataset(\n",
    "    features_df, \"team_position_prediction_npy\",\n",
    "    description=\"a dataset with features for football teams, used for training a model to predict league-position\",\n",
    "    featurestore=featurestore.project_featurestore(),\n",
    "    data_format=\"npy\",\n",
    "    training_dataset_version=featurestore.get_latest_training_dataset_version(\"team_position_prediction_npy\") + 1,\n",
    "    job_name=None,\n",
    "    dependencies=[],\n",
    "    descriptive_statistics=False,\n",
    "    feature_correlation=False,\n",
    "    feature_histograms=False,\n",
    "    cluster_analysis=False,\n",
    "    stat_columns=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save a Training Dataset in petastorm format\n",
    "\n",
    "Petastorm requires an explicit **Unischema** that can be pased to `create_training_dataset` with the optional argument `petastorm_args` that contain a dict of petastorm arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- team_budget: float (nullable = true)\n",
      " |-- average_position: float (nullable = true)\n",
      " |-- sum_player_rating: float (nullable = true)\n",
      " |-- average_attendance: float (nullable = true)\n",
      " |-- average_player_worth: float (nullable = true)\n",
      " |-- sum_player_worth: float (nullable = true)\n",
      " |-- sum_position: float (nullable = true)\n",
      " |-- sum_attendance: float (nullable = true)\n",
      " |-- average_player_rating: float (nullable = true)\n",
      " |-- team_position: integer (nullable = true)\n",
      " |-- sum_player_age: float (nullable = true)\n",
      " |-- average_player_age: float (nullable = true)"
     ]
    }
   ],
   "source": [
    "from petastorm.unischema import dict_to_spark_row, Unischema, UnischemaField\n",
    "from petastorm.codecs import ScalarCodec, CompressedImageCodec, NdarrayCodec\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, FloatType\n",
    "features_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "PetastormSchema = Unischema('team_position_prediction_petastorm_schema', [\n",
    "    UnischemaField('team_budget', np.float32, (), ScalarCodec(FloatType()), False),\n",
    "    UnischemaField('average_position', np.float32, (), ScalarCodec(FloatType()), False),\n",
    "    UnischemaField('sum_player_rating', np.float32, (), ScalarCodec(FloatType()), False),\n",
    "    UnischemaField('average_attendance', np.float32, (), ScalarCodec(FloatType()), False),\n",
    "    UnischemaField('average_player_worth', np.float32, (), ScalarCodec(FloatType()), False),\n",
    "    UnischemaField('sum_player_worth', np.float32, (), ScalarCodec(FloatType()), False),\n",
    "    UnischemaField('sum_position', np.float32, (), ScalarCodec(FloatType()), False),\n",
    "    UnischemaField('average_player_rating', np.float32, (), ScalarCodec(FloatType()), False),\n",
    "    UnischemaField('team_position', np.int32, (), ScalarCodec(IntegerType()), False),\n",
    "    UnischemaField('sum_player_age', np.float32, (), ScalarCodec(FloatType()), False),\n",
    "    UnischemaField('average_player_age', np.float32, (), ScalarCodec(FloatType()), False),\n",
    "])\n",
    "\n",
    "petastorm_args = {\n",
    "    \"schema\": PetastormSchema\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore.create_training_dataset(\n",
    "    features_df, \"team_position_prediction_petastorm\",\n",
    "    description=\"a dataset with features for football teams, used for training a model to predict league-position\",\n",
    "    featurestore=featurestore.project_featurestore(),\n",
    "    data_format=\"petastorm\",\n",
    "    training_dataset_version=featurestore.get_latest_training_dataset_version(\"team_position_prediction_petastorm\") + 1,\n",
    "    job_name=None,\n",
    "    dependencies=[],\n",
    "    descriptive_statistics=False,\n",
    "    feature_correlation=False,\n",
    "    feature_histograms=False,\n",
    "    cluster_analysis=False,\n",
    "    stat_columns=None,\n",
    "    petastorm_args=petastorm_args\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Create a New Version of A Training Dataset\n",
    "\n",
    "To create a new version, simply use the `create_training_dataset` method and specify the version argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore.create_training_dataset(\n",
    "    features_df, \"team_position_prediction\",\n",
    "    training_dataset_version=featurestore.get_latest_training_dataset_version(\"team_position_prediction\") + 1,\n",
    "    descriptive_statistics=False,\n",
    "    feature_correlation=False,\n",
    "    feature_histograms=False,\n",
    "    cluster_analysis=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now see the new version in the feature store UI:\n",
    "\n",
    "![Create Training Dataset Version](./images/create_td_version.png \"Create Training Dataset Version\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inserting Into an Existing Training Dataset\n",
    "\n",
    "Once a dataset have been created, its metadata is browsable in the featurestore registry in the Hopsworks UI. If you don't want to create a new training dataset but just overwrite or insert new data into an existing training dataset, you can use the API function `insert_into_training_dataset`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore.insert_into_training_dataset(\n",
    "    features_df, \n",
    "    \"team_position_prediction_csv\",\n",
    "    descriptive_statistics=False,\n",
    "    feature_correlation=False,\n",
    "    feature_histograms=False,\n",
    "    cluster_analysis=False,\n",
    "    training_dataset_version=featurestore.get_latest_training_dataset_version(\"team_position_prediction_csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default the `insert_into_training_dataset` will use the project's featurestore, overwrite semantics, version 1 of the training dataset, and update the training dataset statistics, this configuration can be overridden.\n",
    "\n",
    "**Note**: \"append\" write mode is not supported for training datasets stored in tfrecords format, only \"overwrite\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore.insert_into_training_dataset(\n",
    "    features_df,\n",
    "    \"team_position_prediction_csv\",\n",
    "    featurestore=featurestore.project_featurestore(),\n",
    "    descriptive_statistics=False,\n",
    "    feature_correlation=False,\n",
    "    feature_histograms=False,\n",
    "    cluster_analysis=False,\n",
    "    stat_columns=None,\n",
    "    write_mode=\"overwrite\",\n",
    "    training_dataset_version=featurestore.get_latest_training_dataset_version(\"team_position_prediction_csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Training Dataset Path\n",
    "\n",
    "After a **managed dataset** have been created, it is easy to share it and re-use it for training various models. For example if the dataset have been materialized in tf-records format you can call the method `get_training_dataset_path(training_dataset)` to get the HDFS path and read it directly in your tensorflow code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'hdfs://10.0.2.15:8020/Projects/demo_featurestore_admin000/demo_featurestore_admin000_Training_Datasets/team_position_prediction_csv_1/team_position_prediction_csv'"
     ]
    }
   ],
   "source": [
    "featurestore.get_training_dataset_path(\"team_position_prediction_csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'hdfs://10.0.2.15:8020/Projects/demo_featurestore_admin000/demo_featurestore_admin000_Training_Datasets/team_position_prediction_hdf5_1/team_position_prediction_hdf5.hdf5'"
     ]
    }
   ],
   "source": [
    "featurestore.get_training_dataset_path(\"team_position_prediction_hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default the library will look for the training dataset in the project's featurestore and use version 1, but this can be overriden if required:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'hdfs://10.0.2.15:8020/Projects/demo_featurestore_admin000/demo_featurestore_admin000_Training_Datasets/team_position_prediction_csv_1/team_position_prediction_csv'"
     ]
    }
   ],
   "source": [
    "featurestore.get_training_dataset_path(\n",
    "    \"team_position_prediction_csv\", \n",
    "    featurestore=featurestore.project_featurestore(),\n",
    "    training_dataset_version=featurestore.get_latest_training_dataset_version(\"team_position_prediction_csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update Training Dataset Stats\n",
    "\n",
    "The API for updating training dataset stats is the same as for updating feature group stats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing descriptive statistics for : team_position_prediction\n",
      "computing feature correlation for: team_position_prediction\n",
      "computing feature histograms for: team_position_prediction\n",
      "computing cluster analysis for: team_position_prediction"
     ]
    }
   ],
   "source": [
    "featurestore.update_training_dataset_stats(\"team_position_prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing descriptive statistics for : team_position_prediction\n",
      "computing feature correlation for: team_position_prediction\n",
      "computing feature histograms for: team_position_prediction\n",
      "computing cluster analysis for: team_position_prediction"
     ]
    }
   ],
   "source": [
    "featurestore.update_training_dataset_stats(\n",
    "    \"team_position_prediction\", \n",
    "    training_dataset_version=1, \n",
    "    featurestore=featurestore.project_featurestore(), \n",
    "    descriptive_statistics=True,\n",
    "    feature_correlation=True, \n",
    "    feature_histograms=True,\n",
    "    cluster_analysis=True,\n",
    "    stat_columns=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read Training Dataset into a Spark Dataframe\n",
    "\n",
    "Typically training datasets are served into deep learning frameworks such as pytorch or tensorflow. However, training datasets can also be read into spark dataframes using the api method `get_training_dataset()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+-----------------+------------------+--------------------+----------------+------------+--------------+---------------------+-------------+--------------+------------------+\n",
      "|team_budget|average_position|sum_player_rating|average_attendance|average_player_worth|sum_player_worth|sum_position|sum_attendance|average_player_rating|team_position|sum_player_age|average_player_age|\n",
      "+-----------+----------------+-----------------+------------------+--------------------+----------------+------------+--------------+---------------------+-------------+--------------+------------------+\n",
      "|  16758.066|           55.15|        32269.797|          3271.934|           307.87268|       30787.268|      1103.0|      65438.68|            322.69797|           26|        2565.0|             25.65|\n",
      "|  3966.3591|            57.1|        29779.197|         4074.8047|           298.78235|       29878.234|      1142.0|      81496.09|            297.79196|           27|        2550.0|              25.5|\n",
      "|  12474.419|           34.35|         88129.83|          9405.213|           888.29443|       88829.445|       687.0|     188104.27|             881.2983|            9|        2578.0|             25.78|\n",
      "|  1621.1936|            40.3|         46779.38|          7118.376|           490.94702|       49094.703|       806.0|     142367.52|             467.7938|           17|        2601.0|             26.01|\n",
      "|    7307.94|           28.15|        131123.84|         19595.973|           1435.2465|       143524.64|       563.0|     391919.47|            1311.2384|            6|        2485.0|             24.85|\n",
      "+-----------+----------------+-----------------+------------------+--------------------+----------------+------------+--------------+---------------------+-------------+--------------+------------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "featurestore.get_training_dataset(\"team_position_prediction_csv\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default the library will read the training dataset from the project's feature store, use version 1 and return the data in a spark dataframe. This can be overriden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  team_budget average_position  ... sum_player_age average_player_age\n",
      "0   16758.066            55.15  ...         2565.0              25.65\n",
      "1   3966.3591             57.1  ...         2550.0               25.5\n",
      "2   12474.419            34.35  ...         2578.0              25.78\n",
      "3   1621.1936             40.3  ...         2601.0              26.01\n",
      "4     7307.94            28.15  ...         2485.0              24.85\n",
      "\n",
      "[5 rows x 12 columns]"
     ]
    }
   ],
   "source": [
    "featurestore.get_training_dataset(\"team_position_prediction_csv\",\n",
    "                                  featurestore=featurestore.project_featurestore(),\n",
    "                                  training_dataset_version=1,\n",
    "                                  dataframe_type=\"pandas\"\n",
    "                                 ).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 12)"
     ]
    }
   ],
   "source": [
    "np_arr = featurestore.get_training_dataset(\"team_position_prediction_hdf5\",\n",
    "                                  dataframe_type=\"numpy\")\n",
    "np_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+-----------------+------------------+--------------------+----------------+------------+--------------+---------------------+-------------+--------------+------------------+\n",
      "|team_budget|average_position|sum_player_rating|average_attendance|average_player_worth|sum_player_worth|sum_position|sum_attendance|average_player_rating|team_position|sum_player_age|average_player_age|\n",
      "+-----------+----------------+-----------------+------------------+--------------------+----------------+------------+--------------+---------------------+-------------+--------------+------------------+\n",
      "|  16758.066|           55.15|        32269.797|          3271.934|           307.87268|       30787.268|      1103.0|      65438.68|            322.69797|           26|        2565.0|             25.65|\n",
      "|  3966.3591|            57.1|        29779.197|         4074.8047|           298.78235|       29878.234|      1142.0|      81496.09|            297.79196|           27|        2550.0|              25.5|\n",
      "|  12474.419|           34.35|         88129.83|          9405.213|           888.29443|       88829.445|       687.0|     188104.27|             881.2983|            9|        2578.0|             25.78|\n",
      "|  1621.1936|            40.3|         46779.38|          7118.376|           490.94702|       49094.703|       806.0|     142367.52|             467.7938|           17|        2601.0|             26.01|\n",
      "|    7307.94|           28.15|        131123.84|         19595.973|           1435.2465|       143524.64|       563.0|     391919.47|            1311.2384|            6|        2485.0|             24.85|\n",
      "+-----------+----------------+-----------------+------------------+--------------------+----------------+------------+--------------+---------------------+-------------+--------------+------------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "featurestore.get_training_dataset(\"team_position_prediction_petastorm\",\n",
    "                                  dataframe_type=\"spark\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Managed Training Dataset Without Using the API \n",
    "\n",
    "To create a **managed** training dataset without using the API, e.g to create a managed training dataset from a .tfrecords file downloaded from Kaggle or from a .csv file from Kaggle, first go to the Feature Registry UI and create a new training dataset and fill in the metadata:\n",
    "\n",
    "![Create Training Dataset From the UI](./images/create_td_1.png \"Create Training Dataset From the UI\")\n",
    "\n",
    "![Create Training Dataset From the UI](./images/create_td_2.png \"Create Training Dataset From the UI\")\n",
    "\n",
    "Once the dataset have been created from the UI, you can find that inside the \"Projectname_Training Datasets\" folder in your project a new folder for the dataset have showed up that is called `training_datasetname_version`:\n",
    "\n",
    "![Create Training Dataset From the UI](./images/create_td_3.png \"Create Training Dataset From the UI\")\n",
    "\n",
    "Simply upload your dataset inside that folder, e.g you can upload for example a single .csv file or a folder with part-r-X.csv files. **It is important that you name the folder/file the name of your training dataset, e.g sample_dataset or sample_dataset.csv**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Featurestore Metadata\n",
    "To explore the contents of the featurestore we recommend using the featurestore page in the Hopsworks UI but you can also get the metadata programmatically from the REST API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update Metadata Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore.get_featurestore_metadata(update_cache=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List all Feature Stores Accessible In the Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['demo_featurestore_admin000_featurestore']"
     ]
    }
   ],
   "source": [
    "featurestore.get_project_featurestores()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List all Feature Groups in a Feature Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['games_features_1', 'season_scores_features_1', 'attendances_features_1', 'players_features_1', 'teams_features_1', 'teams_features_spanish_2', 'pandas_test_example_1', 'python_test_example_1', 'numpy_test_example_1', 'teams_features_spanish_1']"
     ]
    }
   ],
   "source": [
    "featurestore.get_featuregroups()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default `get_featuregroups()` will use the project's feature store, but this can also be specified with the optional argument featurestore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['games_features_1', 'season_scores_features_1', 'attendances_features_1', 'players_features_1', 'teams_features_1', 'teams_features_spanish_2', 'pandas_test_example_1', 'python_test_example_1', 'numpy_test_example_1', 'teams_features_spanish_1']"
     ]
    }
   ],
   "source": [
    "featurestore.get_featuregroups(featurestore=featurestore.project_featurestore())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List all Features in a Feature Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['away_team_id', 'home_team_id', 'score', 'average_position', 'sum_position', 'team_id', 'average_attendance', 'sum_attendance', 'team_id', 'average_player_age', 'average_player_rating', 'average_player_worth', 'sum_player_age', 'sum_player_rating', 'sum_player_worth', 'team_id', 'team_budget', 'team_id', 'team_position', 'equipo_id', 'equipo_posicion', 'equipo_presupuesto', 'average_attendance_test', 'average_player_age_test', 'team_budget_test', 'col_0', 'col_1', 'col_2', 'col_0', 'col_1', 'col_2', 'equipo_id', 'equipo_posicion', 'equipo_presupuesto']"
     ]
    }
   ],
   "source": [
    "featurestore.get_features_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default get_features_list() will use the project's feature store, but this can also be specified with the optional argument featurestore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['away_team_id', 'home_team_id', 'score', 'average_position', 'sum_position', 'team_id', 'average_attendance', 'sum_attendance', 'team_id', 'average_player_age', 'average_player_rating', 'average_player_worth', 'sum_player_age', 'sum_player_rating', 'sum_player_worth', 'team_id', 'team_budget', 'team_id', 'team_position', 'equipo_id', 'equipo_posicion', 'equipo_presupuesto', 'average_attendance_test', 'average_player_age_test', 'team_budget_test', 'col_0', 'col_1', 'col_2', 'col_0', 'col_1', 'col_2', 'equipo_id', 'equipo_posicion', 'equipo_presupuesto']"
     ]
    }
   ],
   "source": [
    "featurestore.get_features_list(featurestore=featurestore.project_featurestore())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List all Training Datasets in a Feature Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['team_position_prediction_1', 'team_position_prediction_csv_1', 'team_position_prediction_tsv_1', 'team_position_prediction_parquet_1', 'team_position_prediction_orc_1', 'team_position_prediction_avro_1', 'team_position_prediction_hdf5_1', 'team_position_prediction_npy_1', 'team_position_prediction_petastorm_1', 'team_position_prediction_2']"
     ]
    }
   ],
   "source": [
    "featurestore.get_training_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default `get_training_datasets()` will use the project's feature store, but this can also be specified with the optional argument featurestore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['team_position_prediction_1', 'team_position_prediction_csv_1', 'team_position_prediction_tsv_1', 'team_position_prediction_parquet_1', 'team_position_prediction_orc_1', 'team_position_prediction_avro_1', 'team_position_prediction_hdf5_1', 'team_position_prediction_npy_1', 'team_position_prediction_petastorm_1', 'team_position_prediction_2']"
     ]
    }
   ],
   "source": [
    "featurestore.get_training_datasets(featurestore=featurestore.project_featurestore())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get All Metadata (Features, Feature groups, Training Datasets) for a Feature Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore.get_featurestore_metadata()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default `get_featurestore_metadata` will use the project's feature store, but this can also be specified with the optional argument featurestore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore.get_featurestore_metadata(featurestore=featurestore.project_featurestore())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Raw Data to Features to Training Dataset to Model\n",
    "\n",
    "Once a training dataset have been materialized, we can use it to train a model. The featurestore API makes the integration with libraries such as Tensorflow simple. In this section we will train an example model using the training dataset `team_position_prediction` that we created earlier. We will use the column **\"team_position\"** as the target to predict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get TFRecords Schema from a Spark Dataframe\n",
    "\n",
    "This utility method can be used when parsing training datasets in the tfrecords format. Note that this method will try to infer the tensorflow example schema from the schema of the spark dataframe. If you want full control of the tf-record schema you should define it manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'team_budget': FixedLenFeature(shape=[], dtype=tf.float32, default_value=None), 'average_position': FixedLenFeature(shape=[], dtype=tf.float32, default_value=None), 'sum_player_rating': FixedLenFeature(shape=[], dtype=tf.float32, default_value=None), 'average_attendance': FixedLenFeature(shape=[], dtype=tf.float32, default_value=None), 'average_player_worth': FixedLenFeature(shape=[], dtype=tf.float32, default_value=None), 'sum_player_worth': FixedLenFeature(shape=[], dtype=tf.float32, default_value=None), 'sum_position': FixedLenFeature(shape=[], dtype=tf.float32, default_value=None), 'sum_attendance': FixedLenFeature(shape=[], dtype=tf.float32, default_value=None), 'average_player_rating': FixedLenFeature(shape=[], dtype=tf.float32, default_value=None), 'team_position': FixedLenFeature(shape=[], dtype=tf.int64, default_value=None), 'sum_player_age': FixedLenFeature(shape=[], dtype=tf.float32, default_value=None), 'average_player_age': FixedLenFeature(shape=[], dtype=tf.float32, default_value=None)}"
     ]
    }
   ],
   "source": [
    "featurestore.get_dataframe_tf_record_schema(features_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get TFRecords Schema from a Training Dataset\n",
    "\n",
    "When a training dataset is saved in the tfrecords format, the tfrecord schema is stored together with the training dataset in a file called `tf_record_schema.txt`. This schema can be retrieved when the tfrecords need to be parsed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'team_budget': FixedLenFeature(shape=[], dtype=tf.float32, default_value=None), 'average_position': FixedLenFeature(shape=[], dtype=tf.float32, default_value=None), 'sum_player_rating': FixedLenFeature(shape=[], dtype=tf.float32, default_value=None), 'average_attendance': FixedLenFeature(shape=[], dtype=tf.float32, default_value=None), 'average_player_worth': FixedLenFeature(shape=[], dtype=tf.float32, default_value=None), 'sum_player_worth': FixedLenFeature(shape=[], dtype=tf.float32, default_value=None), 'sum_position': FixedLenFeature(shape=[], dtype=tf.float32, default_value=None), 'sum_attendance': FixedLenFeature(shape=[], dtype=tf.float32, default_value=None), 'average_player_rating': FixedLenFeature(shape=[], dtype=tf.float32, default_value=None), 'team_position': FixedLenFeature(shape=[], dtype=tf.int64, default_value=None), 'sum_player_age': FixedLenFeature(shape=[], dtype=tf.float32, default_value=None), 'average_player_age': FixedLenFeature(shape=[], dtype=tf.float32, default_value=None)}"
     ]
    }
   ],
   "source": [
    "featurestore.get_training_dataset_tf_record_schema(\"team_position_prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default `get_training_dataset_tf_record_schema` will use the project's feature store and version 1 of the feature grup, this can be overriden if necessary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'team_budget': FixedLenFeature(shape=[], dtype=tf.float32, default_value=None), 'average_position': FixedLenFeature(shape=[], dtype=tf.float32, default_value=None), 'sum_player_rating': FixedLenFeature(shape=[], dtype=tf.float32, default_value=None), 'average_attendance': FixedLenFeature(shape=[], dtype=tf.float32, default_value=None), 'average_player_worth': FixedLenFeature(shape=[], dtype=tf.float32, default_value=None), 'sum_player_worth': FixedLenFeature(shape=[], dtype=tf.float32, default_value=None), 'sum_position': FixedLenFeature(shape=[], dtype=tf.float32, default_value=None), 'sum_attendance': FixedLenFeature(shape=[], dtype=tf.float32, default_value=None), 'average_player_rating': FixedLenFeature(shape=[], dtype=tf.float32, default_value=None), 'team_position': FixedLenFeature(shape=[], dtype=tf.int64, default_value=None), 'sum_player_age': FixedLenFeature(shape=[], dtype=tf.float32, default_value=None), 'average_player_age': FixedLenFeature(shape=[], dtype=tf.float32, default_value=None)}"
     ]
    }
   ],
   "source": [
    "featurestore.get_training_dataset_tf_record_schema(\n",
    "    \"team_position_prediction\", \n",
    "    training_dataset_version=1,\n",
    "    featurestore = featurestore.project_featurestore()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "In this example we will use Tensorflow and Keras. However, the feature store is in theory agnostic to which framework or method you use for training the model, it works with PyTorch, spark-mllib etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd\n",
    "import json\n",
    "from hops import hdfs\n",
    "from hops import experiment\n",
    "from tensorflow.python.keras.callbacks import TensorBoard\n",
    "from hops import tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants and HyperParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 10\n",
    "SHUFFLE_BUFFER_SIZE = 10000\n",
    "INPUT_SHAPE = 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse TFRecords into A TF-Dataset\n",
    "\n",
    "The dataset is stored in `.tfrecords` format, which essentially means it is stored in protobuf format. Moreover, to be able to read and write datasets in the petabyte-scale, the feature store uses distrbuted write/read to HopsFS with Spark, so the dataset is spread out in a large number of files prefixed with `part-r` (if you are unfamiliar with Spark partitions you can read up on Spark [here](https://spark.apache.org/docs/2.1.0/programming-guide.html)). \n",
    "\n",
    "Despite that our dataset is stored in a binary format and stored disributed in HopsFSs the amount of code to read the data into a tensorflow dataset is very little, thanks to \n",
    "\n",
    "- `featurestore.et_training_dataset_path`: gets the path in HopsFS where the tfrecords files are stored \n",
    "- `featurestore.get_training_dataset_tf_record_schema`: gets the tf-record schema to parse the binary data\n",
    "- `tf.gfile.Glob`: Gets a list of file names from a file-pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_dataset():\n",
    "    dataset_dir = featurestore.get_training_dataset_path(\"team_position_prediction\")\n",
    "    input_files = tf.gfile.Glob(dataset_dir + \"/part-r-*\")\n",
    "    dataset = tf.data.TFRecordDataset(input_files)\n",
    "    tf_record_schema = featurestore.get_training_dataset_tf_record_schema(\"team_position_prediction\")\n",
    "    feature_names = [\"team_budget\", \"average_attendance\", \"average_player_age\", \"sum_attendance\", \n",
    "         \"average_player_rating\", \"average_player_worth\", \"sum_player_age\", \"sum_player_rating\", \"sum_player_worth\", \n",
    "         \"sum_position\", \"average_position\"\n",
    "        ]\n",
    "    label_name = \"team_position\"\n",
    "\n",
    "    def decode(example_proto):\n",
    "        example = tf.parse_single_example(example_proto, tf_record_schema)\n",
    "        x = []\n",
    "        for feature_name in feature_names:\n",
    "            x.append(example[feature_name])\n",
    "        y = [tf.cast(example[label_name], tf.float32)]\n",
    "        return x,y\n",
    "\n",
    "    dataset = dataset.map(decode).shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE).repeat(NUM_EPOCHS)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define The Model using Keras and Tensorflow \n",
    "\n",
    "We will use a three-layer neural network for regression on our dataset. In this tutorial we work with so little data that using a larger model does not make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(num_neurons = 64, learning_rate = 0.001):\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Dense(num_neurons, activation='relu', \n",
    "                     input_shape = (INPUT_SHAPE,),\n",
    "                    batch_size=BATCH_SIZE),\n",
    "        layers.Dense(num_neurons, activation='relu'),\n",
    "        layers.Dense(1)])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Train Function\n",
    "\n",
    "We define the train code in a separate function so that it can be distributed in the cluster across different executors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(num_neurons_per_layer, learning_rate):\n",
    "    dataset = create_tf_dataset()\n",
    "    model = create_model()\n",
    "    model.compile(optimizer=tf.train.AdamOptimizer(learning_rate), loss='mse', metrics=['accuracy'])\n",
    "    tb_callback = TensorBoard(log_dir=tensorboard.logdir(), histogram_freq=0,\n",
    "                             write_graph=True, write_images=True)\n",
    "    callbacks = [tb_callback]\n",
    "    callbacks.append(keras.callbacks.ModelCheckpoint(tensorboard.logdir() + '/checkpoint-{epoch}.h5',\n",
    "                                                    monitor='acc', verbose=0, save_best_only=True))\n",
    "    history = model.fit(dataset, epochs=NUM_EPOCHS, steps_per_epoch = 5, callbacks=callbacks)\n",
    "    #Dump training history to HDFS so we can use it later on for analysis\n",
    "    results_path = hdfs.project_path() + \"Logs/featurestore_tour_model_results_loss.txt\"\n",
    "    hdfs.dump(json.dumps(history.history), results_path)\n",
    "    #Return experiment metric (used to direct the search when using hyperparameter search)\n",
    "    return history.history[\"acc\"][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Search \n",
    "\n",
    "We will create a reproducible experiment to search for the best hyperparameters for our model using the hops `Ã¨xperiment` module and evolutionary search. \n",
    "\n",
    "When the experiment is running you can view the progress in the SparkUI, from there you can also find the tensorboards for the executors:\n",
    "\n",
    "![Open Spark UI to monitor Experiment 1](./images/open_sparkui_0.png \"Open Spark Ui to Monitor Experiment 1\")\n",
    "\n",
    "![Open Spark UI to monitor Experiment 2](./images/open_sparkui_2.png \"Open Spark Ui to Monitor Experiment 2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 0 || average metric: 0.010000000149011612, best metric: 0.020000000298023225, best parameter combination: ['num_neurons_per_layer=119', 'learning_rate=0.009920993413917258']\n",
      "\n",
      "Generation 1 || average metric: 0.01500000022351742, best metric: 0.020000000298023225, best parameter combination: ['num_neurons_per_layer=119', 'learning_rate=0.009920993413917258']\n",
      "\n",
      "Finished Experiment"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 5\n",
    "search_dict = {\"num_neurons_per_layer\" : [64,128], \"learning_rate\": [0.001, 0.01]}\n",
    "log_dir, best_params = experiment.differential_evolution(\n",
    "    train_fn, \n",
    "    search_dict, \n",
    "    name='team_position_prediction_hyperparam_search', \n",
    "    description='Evolutionary search through the search space of hyperparameters with parallel executors to find the best parameters',\n",
    "    local_logdir=True, \n",
    "    population=4,\n",
    "    generations = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewing old Experiments\n",
    "\n",
    "To view the result of an old experiment, go to the \"Experiments\" tab.\n",
    "\n",
    "![Open Spark UI to monitor Experiment 3](./images/open_sparkui_3.png \"Open Spark Ui to Monitor Experiment 3\")\n",
    "\n",
    "![Open Spark UI to monitor Experiment 4](./images/open_sparkui_4.png \"Open Spark Ui to Monitor Experiment 4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with the Best Hyperparameters\n",
    "\n",
    "Now we can train for longer amount of epochs when we have found the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_neurons_per_layer': '119', 'learning_rate': '0.009920993413917258'}"
     ]
    }
   ],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_d = {}\n",
    "args_d[\"num_neurons_per_layer\"] = [int(best_params[\"num_neurons_per_layer\"])]\n",
    "args_d[\"learning_rate\"] = [float(best_params[\"learning_rate\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Experiment"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 40\n",
    "experiment_result_path = experiment.launch(\n",
    "    train_fn, \n",
    "    args_dict = args_d,\n",
    "    name='team_position_prediction_hyperparam_search',\n",
    "    description=\"experiment to train model for team position prediction\",\n",
    "    local_logdir=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'hdfs://10.0.2.15:8020/Projects/demo_featurestore_admin000/Experiments/application_1550835076939_0007/launcher/run.1'"
     ]
    }
   ],
   "source": [
    "experiment_result_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Experiment Results\n",
    "\n",
    "To view the experiment result, go again to the \"Experiments tab\" and open the experiment you just ran, just the same way as you opened the hyperparameter-search experiment after it had finished (copy the application-id to the search bar to open tensorboard).\n",
    "\n",
    "![Open Spark UI to monitor Experiment 5](./images/open_sparkui_5.png \"Open Spark Ui to Monitor Experiment 5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Results\n",
    "\n",
    "In the `train_fn` function we store some training history to HDFS. We can use that history to do custom plots in addition to the tensorboard that can be opened from the **Experiments** tab.\n",
    "\n",
    "The code in this notebook is executed inside the Hops cluster, we can load the results into the local machine for plotting. To read more about the setup with jupyter notebooks on Hopsworks, look [here](https://hops.readthedocs.io/en/latest/user_guide/hopsworks/jupyter.html#plotting-with-pyspark-kernel).\n",
    "\n",
    "We are going to use matplotlib for plotting, if you have not already installed it in your project, you can do so from the python-tab in the project UI.\n",
    "\n",
    "![Install plt](./images/install_plt.png \"Install plt\")\n",
    "\n",
    "If you just installed matplotlib, you have to restart the Jupyter kernel for the changes to take effect. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fe3eb37c7b8>]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%local\n",
    "%matplotlib inline\n",
    "import json\n",
    "from hops import hdfs\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 15, 10\n",
    "results_path = hdfs.project_path() + \"Logs/featurestore_tour_model_results_loss.txt\"\n",
    "results = json.loads(hdfs.load(results_path))\n",
    "y = results[\"loss\"] #loss\n",
    "x = list(range(1, len(y)+1))#epoch\n",
    "plt.title(\"Loss per Epoch - Football team Position Prediction\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}